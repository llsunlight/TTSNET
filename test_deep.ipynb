{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from scipy.io import loadmat\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from h5py import File\n",
    "from scripts.dataset import Dataset\n",
    "from scripts.model import EEGNet, OTSNet, OTSLoss, TTSNet, TTSLoss\n",
    "from skorch import NeuralNetClassifier, NeuralNet\n",
    "from skorch.callbacks import EpochScoring\n",
    "from skorch.callbacks.scoring import ScoringBase\n",
    "# from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochScoringonTest(ScoringBase):\n",
    "    def __init__(self, test_data, test_label, \n",
    "                scoring,\n",
    "                lower_is_better=False,\n",
    "                on_train=False,\n",
    "                name=None,\n",
    "                use_caching=False) -> None:\n",
    "        super(EpochScoringonTest, self).__init__(\n",
    "            scoring=scoring,\n",
    "            lower_is_better=lower_is_better, \n",
    "            on_train=on_train,\n",
    "            name=name, \n",
    "            use_caching=use_caching)\n",
    "        self.test_data = test_data\n",
    "        self.test_label = test_label\n",
    "\n",
    "    def _record_score(self, history, current_score):\n",
    "        \"\"\"Record the current store and, if applicable, if it's the best score\n",
    "        yet.\n",
    "\n",
    "        \"\"\"\n",
    "        history.record(self.name_, current_score)\n",
    "\n",
    "        is_best = self._is_best_score(current_score)\n",
    "        if is_best is None:\n",
    "            return\n",
    "\n",
    "        history.record(self.name_ + '_best', bool(is_best))\n",
    "        if is_best:\n",
    "            self.best_score_ = current_score\n",
    "\n",
    "    # pylint: disable=unused-argument\n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        net.training = False\n",
    "        current_score = net.score(self.test_data, self.test_label)\n",
    "        self._record_score(net.history, current_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sub0::   0%|          | 0/5 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key 'test_acc' was not found in history.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39mset_params(train_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     39\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_set, train_label)\n\u001b[0;32m---> 40\u001b[0m accuracy_test[sub,fold,:] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_acc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/history.py:291\u001b[0m, in \u001b[0;36mHistory.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    287\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(_not_none, items))\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m items \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (k_e \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatches\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i_b \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# none of the epochs matched\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(keyerror_msg\u001b[38;5;241m.\u001b[39mformat(key))\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(i_b, \u001b[38;5;28mslice\u001b[39m)\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m k_b \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(batches \u001b[38;5;28;01mfor\u001b[39;00m batches \u001b[38;5;129;01min\u001b[39;00m items)\n\u001b[1;32m    297\u001b[0m ):\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# none of the batches matched\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(keyerror_msg\u001b[38;5;241m.\u001b[39mformat(key))\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'test_acc' was not found in history.\""
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    Fs = 256\n",
    "    num_fold = 5\n",
    "    num_subj = 10\n",
    "    num_epoch= 500\n",
    "    num_class= 5\n",
    "    lr_rate = 0.002\n",
    "    max_freq = 10\n",
    "    step_bank= 2\n",
    "    frange = np.arange(step_bank,max_freq+1,step_bank)\n",
    "    channels = np.arange(62)#(np.int64([1,3,5,15,29,30,31,45,55,57,59])-1).tolist()\n",
    "    num_bank = int(max_freq/step_bank)\n",
    "    t  = [0, 2]\n",
    "    accuracy_test  = np.zeros([num_subj, num_fold, num_epoch])\n",
    "    torch.manual_seed(1169456885)\n",
    "    np.random.seed(1169456885)\n",
    "    for sub in range(num_subj):\n",
    "        dataset=Dataset(num_class=num_class,path='/Volumes/我是硬盘/Dataset/OData/Dataset II/', channel=channels)\n",
    "        dataset.load_data(sub, 'zscore')\n",
    "        dataset.divide_data(num_fold)\n",
    "        dataset.filter_data(fs=Fs, frange=frange, order=8)\n",
    "        for fold in tqdm(range(num_fold), desc=f'Sub{sub}:'):\n",
    "            train_set, train_label, test_set, test_label = dataset.retreive_data(\n",
    "                    class_index=np.arange(num_class), fold_index=fold, filter_index=np.arange(num_bank)+1, precison='*')\n",
    "            train_set = np.transpose(train_set[:,:,int(t[0]*Fs):int(t[1]*Fs),:], (0,3,1,2))\n",
    "            test_set  = np.transpose(test_set[:,:,int(t[0]*Fs):int(t[1]*Fs), :], (0,3,1,2))\n",
    "            net = TTSNet(num_classes=num_class, num_samples=int(t[1]-t[0])*Fs, num_fbanks=num_bank, num_channels=len(channels))\n",
    "            model = NeuralNetClassifier(\n",
    "                net,\n",
    "                max_epochs=num_epoch,\n",
    "                criterion=TTSLoss(),\n",
    "                lr=lr_rate,\n",
    "                optimizer=torch.optim.Adam,\n",
    "                device='cpu',\n",
    "                callbacks=[EpochScoringonTest(test_set, test_label, scoring='accuracy', name='test_acc',\n",
    "                                              on_train=False)]\n",
    "            )\n",
    "            model.set_params(train_split=None, verbose=0)\n",
    "            model.fit(train_set, train_label)\n",
    "            accuracy_test[sub,fold,:] = model.history[:, 'test_acc']\n",
    "            # result[sub,fold] =model.score(test_set, test_label)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sub0::   0%|          | 0/5 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (16x1x4). Calculated output size: (16x1x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m\n",
      "\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralNetClassifier(\n",
      "\u001b[1;32m     29\u001b[0m     net,\n",
      "\u001b[1;32m     30\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mnum_epoch,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     36\u001b[0m                                   on_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n",
      "\u001b[1;32m     37\u001b[0m )\n",
      "\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39mset_params(train_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;32m---> 39\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_label\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     40\u001b[0m accuracy_test[sub,fold,:] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mhistory[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/classifier.py:165\u001b[0m, in \u001b[0;36mNeuralNetClassifier.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n",
      "\u001b[1;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See ``NeuralNet.fit``.\u001b[39;00m\n",
      "\u001b[1;32m    155\u001b[0m \n",
      "\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    160\u001b[0m \n",
      "\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# pylint: disable=useless-super-delegation\u001b[39;00m\n",
      "\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# this is actually a pylint bug:\u001b[39;00m\n",
      "\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# https://github.com/PyCQA/pylint/issues/1085\u001b[39;00m\n",
      "\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNeuralNetClassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1319\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n",
      "\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized_:\n",
      "\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1278\u001b[0m, in \u001b[0;36mNeuralNet.partial_fit\u001b[0;34m(self, X, y, classes, **fit_params)\u001b[0m\n",
      "\u001b[1;32m   1276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_train_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my)\n",
      "\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1190\u001b[0m, in \u001b[0;36mNeuralNet.fit_loop\u001b[0;34m(self, X, y, epochs, **fit_params)\u001b[0m\n",
      "\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_epoch_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mon_epoch_kwargs)\n",
      "\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1191\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single_epoch(iterator_valid, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m   1194\u001b[0m                           step_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_step, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mon_epoch_kwargs)\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1226\u001b[0m, in \u001b[0;36mNeuralNet.run_single_epoch\u001b[0;34m(self, iterator, training, prefix, step_fn, **fit_params)\u001b[0m\n",
      "\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iterator:\n",
      "\u001b[1;32m   1225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_batch_begin\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch\u001b[38;5;241m=\u001b[39mbatch, training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "\u001b[0;32m-> 1226\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[43mstep_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mrecord_batch(prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, step[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;32m   1228\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m (get_len(batch[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m))\n",
      "\u001b[1;32m   1229\u001b[0m                   \u001b[38;5;28;01melse\u001b[39;00m get_len(batch))\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1105\u001b[0m, in \u001b[0;36mNeuralNet.train_step\u001b[0;34m(self, batch, **fit_params)\u001b[0m\n",
      "\u001b[1;32m   1097\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\n",
      "\u001b[1;32m   1098\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_grad_computed\u001b[39m\u001b[38;5;124m'\u001b[39m,\n",
      "\u001b[1;32m   1099\u001b[0m         named_parameters\u001b[38;5;241m=\u001b[39mTeeGenerator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_all_learnable_params()),\n",
      "\u001b[1;32m   1100\u001b[0m         batch\u001b[38;5;241m=\u001b[39mbatch,\n",
      "\u001b[1;32m   1101\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[1;32m   1102\u001b[0m     )\n",
      "\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;32m-> 1105\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_accumulator\u001b[38;5;241m.\u001b[39mget_step()\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1060\u001b[0m, in \u001b[0;36mNeuralNet._step_optimizer\u001b[0;34m(self, step_fn)\u001b[0m\n",
      "\u001b[1;32m   1058\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1060\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n",
      "\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    482\u001b[0m             )\n",
      "\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n",
      "\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/optim/adam.py:205\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n",
      "\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n",
      "\u001b[0;32m--> 205\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n",
      "\u001b[1;32m    208\u001b[0m     params_with_grad: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1094\u001b[0m, in \u001b[0;36mNeuralNet.train_step.<locals>.step_fn\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_fn\u001b[39m():\n",
      "\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_optimizer()\n",
      "\u001b[0;32m-> 1094\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1095\u001b[0m     step_accumulator\u001b[38;5;241m.\u001b[39mstore_step(step)\n",
      "\u001b[1;32m   1097\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\n",
      "\u001b[1;32m   1098\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_grad_computed\u001b[39m\u001b[38;5;124m'\u001b[39m,\n",
      "\u001b[1;32m   1099\u001b[0m         named_parameters\u001b[38;5;241m=\u001b[39mTeeGenerator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_all_learnable_params()),\n",
      "\u001b[1;32m   1100\u001b[0m         batch\u001b[38;5;241m=\u001b[39mbatch,\n",
      "\u001b[1;32m   1101\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[1;32m   1102\u001b[0m     )\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:993\u001b[0m, in \u001b[0;36mNeuralNet.train_step_single\u001b[0;34m(self, batch, **fit_params)\u001b[0m\n",
      "\u001b[1;32m    991\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_training(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;32m    992\u001b[0m Xi, yi \u001b[38;5;241m=\u001b[39m unpack_data(batch)\n",
      "\u001b[0;32m--> 993\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    994\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_loss(y_pred, yi, X\u001b[38;5;241m=\u001b[39mXi, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;32m    995\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1521\u001b[0m, in \u001b[0;36mNeuralNet.infer\u001b[0;34m(self, x, **fit_params)\u001b[0m\n",
      "\u001b[1;32m   1519\u001b[0m     x_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_x_and_fit_params(x, fit_params)\n",
      "\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mx_dict)\n",
      "\u001b[0;32m-> 1521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Downloads/TDLDA_我的/scripts/model.py:563\u001b[0m, in \u001b[0;36mTTSNet.forward\u001b[0;34m(self, x)\u001b[0m\n",
      "\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
      "\u001b[0;32m--> 563\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock1[i](x[:,i,:,:]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock1))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;66;03m#增加池化层以后不需要通过标签提取\u001b[39;00m\n",
      "\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "\u001b[1;32m    565\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock2(x)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/Downloads/TDLDA_我的/scripts/model.py:563\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
      "\u001b[0;32m--> 563\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock1))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;66;03m#增加池化层以后不需要通过标签提取\u001b[39;00m\n",
      "\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "\u001b[1;32m    565\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock2(x)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Downloads/TDLDA_我的/scripts/model.py:455\u001b[0m, in \u001b[0;36mEEGNet.forward\u001b[0;34m(self, x)\u001b[0m\n",
      "\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
      "\u001b[0;32m--> 455\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    456\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock2(x)\n",
      "\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/pooling.py:641\u001b[0m, in \u001b[0;36mAvgPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "\u001b[0;32m--> 641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    642\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_include_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivisor_override\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (16x1x4). Calculated output size: (16x1x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    Fs = 256\n",
    "    num_fold = 5\n",
    "    num_subj = 10\n",
    "    num_epoch= 500\n",
    "    num_class= 5\n",
    "    lr_rate = 0.002\n",
    "    max_freq = 10\n",
    "    step_bank= 2\n",
    "    frange = np.arange(step_bank,max_freq+1,step_bank)\n",
    "    channels = np.arange(62)#(np.int64([1,3,5,15,29,30,31,45,55,57,59])-1).tolist()\n",
    "    num_bank = int(max_freq/step_bank)\n",
    "    t  = [0, 2]\n",
    "    accuracy_test  = np.zeros([num_subj, num_fold, num_epoch])\n",
    "    torch.manual_seed(1169456885)\n",
    "    np.random.seed(1169456885)\n",
    "    for sub in range(num_subj):\n",
    "        dataset=Dataset(num_class=num_class,path='/Volumes/我是硬盘/Dataset/OData/Dataset II/', channel=channels)\n",
    "        dataset.load_data(sub, 'zscore')\n",
    "        dataset.divide_data(num_fold)\n",
    "        dataset.filter_data(fs=Fs, frange=frange, order=8)\n",
    "        for fold in tqdm(range(num_fold), desc=f'Sub{sub}:'):\n",
    "            train_set, train_label, test_set, test_label = dataset.retreive_data(\n",
    "                    class_index=np.arange(num_class), fold_index=fold, filter_index=np.arange(num_bank)+1, precison='*')\n",
    "            train_set = np.transpose(train_set[:,:,int(t[0]*Fs):int(t[1]*Fs),:], (0,3,1,2))\n",
    "            test_set  = np.transpose(test_set[:,:,int(t[0]*Fs):int(t[1]*Fs), :], (0,3,1,2))\n",
    "            net = TTSNet(num_classes=num_class, num_samples=int(t[1]-t[0])*Fs, num_fbanks=num_bank, num_channels=len(channels))\n",
    "            model = NeuralNetClassifier(\n",
    "                net,\n",
    "                max_epochs=num_epoch,\n",
    "                criterion=TTSLoss(),\n",
    "                lr=lr_rate,\n",
    "                optimizer=torch.optim.Adam,\n",
    "                device='cpu',\n",
    "                callbacks=[EpochScoringonTest(test_set, test_label, scoring='accuracy', name='test_acc',\n",
    "                                              on_train=False)]\n",
    "            )\n",
    "            model.set_params(train_split=None, verbose=0)\n",
    "            model.fit(train_set, train_label)\n",
    "            accuracy_test[sub,fold,:] = model.history[:, 'test_acc']\n",
    "            # result[sub,fold] =model.score(test_set, test_label)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sub0::   0%|          | 0/5 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (16x1x4). Calculated output size: (16x1x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralNetClassifier(\n\u001b[1;32m     29\u001b[0m     net,\n\u001b[1;32m     30\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mnum_epoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m                                   on_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39mset_params(train_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m accuracy_test[sub,fold,:] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mhistory[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/classifier.py:165\u001b[0m, in \u001b[0;36mNeuralNetClassifier.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See ``NeuralNet.fit``.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# pylint: disable=useless-super-delegation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# this is actually a pylint bug:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# https://github.com/PyCQA/pylint/issues/1085\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNeuralNetClassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1319\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized_:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1278\u001b[0m, in \u001b[0;36mNeuralNet.partial_fit\u001b[0;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_train_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1190\u001b[0m, in \u001b[0;36mNeuralNet.fit_loop\u001b[0;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_epoch_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mon_epoch_kwargs)\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single_epoch(iterator_valid, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1194\u001b[0m                           step_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_step, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mon_epoch_kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1226\u001b[0m, in \u001b[0;36mNeuralNet.run_single_epoch\u001b[0;34m(self, iterator, training, prefix, step_fn, **fit_params)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_batch_begin\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch\u001b[38;5;241m=\u001b[39mbatch, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m-> 1226\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[43mstep_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mrecord_batch(prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, step[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m   1228\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m (get_len(batch[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m))\n\u001b[1;32m   1229\u001b[0m                   \u001b[38;5;28;01melse\u001b[39;00m get_len(batch))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1105\u001b[0m, in \u001b[0;36mNeuralNet.train_step\u001b[0;34m(self, batch, **fit_params)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\n\u001b[1;32m   1098\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_grad_computed\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1099\u001b[0m         named_parameters\u001b[38;5;241m=\u001b[39mTeeGenerator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_all_learnable_params()),\n\u001b[1;32m   1100\u001b[0m         batch\u001b[38;5;241m=\u001b[39mbatch,\n\u001b[1;32m   1101\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1102\u001b[0m     )\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 1105\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_accumulator\u001b[38;5;241m.\u001b[39mget_step()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1060\u001b[0m, in \u001b[0;36mNeuralNet._step_optimizer\u001b[0;34m(self, step_fn)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/optim/adam.py:205\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 205\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    208\u001b[0m     params_with_grad: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1094\u001b[0m, in \u001b[0;36mNeuralNet.train_step.<locals>.step_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_fn\u001b[39m():\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_optimizer()\n\u001b[0;32m-> 1094\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1095\u001b[0m     step_accumulator\u001b[38;5;241m.\u001b[39mstore_step(step)\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\n\u001b[1;32m   1098\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_grad_computed\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1099\u001b[0m         named_parameters\u001b[38;5;241m=\u001b[39mTeeGenerator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_all_learnable_params()),\n\u001b[1;32m   1100\u001b[0m         batch\u001b[38;5;241m=\u001b[39mbatch,\n\u001b[1;32m   1101\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1102\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:993\u001b[0m, in \u001b[0;36mNeuralNet.train_step_single\u001b[0;34m(self, batch, **fit_params)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_training(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    992\u001b[0m Xi, yi \u001b[38;5;241m=\u001b[39m unpack_data(batch)\n\u001b[0;32m--> 993\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    994\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_loss(y_pred, yi, X\u001b[38;5;241m=\u001b[39mXi, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    995\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/skorch/net.py:1521\u001b[0m, in \u001b[0;36mNeuralNet.infer\u001b[0;34m(self, x, **fit_params)\u001b[0m\n\u001b[1;32m   1519\u001b[0m     x_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_x_and_fit_params(x, fit_params)\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mx_dict)\n\u001b[0;32m-> 1521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/TDLDA_我的/scripts/model.py:563\u001b[0m, in \u001b[0;36mTTSNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 563\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock1[i](x[:,i,:,:]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock1))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;66;03m#增加池化层以后不需要通过标签提取\u001b[39;00m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    565\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock2(x)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/TDLDA_我的/scripts/model.py:563\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 563\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock1))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;66;03m#增加池化层以后不需要通过标签提取\u001b[39;00m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    565\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock2(x)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/TDLDA_我的/scripts/model.py:455\u001b[0m, in \u001b[0;36mEEGNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 455\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock2(x)\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/pooling.py:641\u001b[0m, in \u001b[0;36mAvgPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_include_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivisor_override\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (16x1x4). Calculated output size: (16x1x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    Fs = 256\n",
    "    num_fold = 5\n",
    "    num_subj = 10\n",
    "    num_epoch= 500\n",
    "    num_class= 5\n",
    "    lr_rate = 0.002\n",
    "    max_freq = 10\n",
    "    step_bank= 2\n",
    "    frange = np.arange(step_bank,max_freq+1,step_bank)\n",
    "    channels = np.arange(62)#(np.int64([1,3,5,15,29,30,31,45,55,57,59])-1).tolist()\n",
    "    num_bank = int(max_freq/step_bank)\n",
    "    t  = [0, 2]\n",
    "    accuracy_test  = np.zeros([num_subj, num_fold, num_epoch])\n",
    "    torch.manual_seed(1169456885)\n",
    "    np.random.seed(1169456885)\n",
    "    for sub in range(num_subj):\n",
    "        dataset=Dataset(num_class=num_class,path='/Volumes/我是硬盘/Dataset/OData/Dataset II/', channel=channels)\n",
    "        dataset.load_data(sub, 'zscore')\n",
    "        dataset.divide_data(num_fold)\n",
    "        dataset.filter_data(fs=Fs, frange=frange, order=8)\n",
    "        for fold in tqdm(range(num_fold), desc=f'Sub{sub}:'):\n",
    "            train_set, train_label, test_set, test_label = dataset.retreive_data(\n",
    "                    class_index=np.arange(num_class), fold_index=fold, filter_index=np.arange(num_bank)+1, precison='*')\n",
    "            train_set = np.transpose(train_set[:,:,int(t[0]*Fs):int(t[1]*Fs),:], (0,3,1,2))\n",
    "            test_set  = np.transpose(test_set[:,:,int(t[0]*Fs):int(t[1]*Fs), :], (0,3,1,2))\n",
    "            net = TTSNet(num_classes=num_class, num_samples=int(t[1]-t[0])*Fs, num_fbanks=num_bank, num_channels=len(channels))\n",
    "            model = NeuralNetClassifier(\n",
    "                net,\n",
    "                max_epochs=num_epoch,\n",
    "                criterion=TTSLoss(),\n",
    "                lr=lr_rate,\n",
    "                optimizer=torch.optim.Adam,\n",
    "                device='cpu',\n",
    "                callbacks=[EpochScoringonTest(test_set, test_label, scoring='accuracy', name='test_acc',\n",
    "                                              on_train=False)]\n",
    "            )\n",
    "            model.set_params(train_split=None, verbose=0)\n",
    "            model.fit(train_set, train_label)\n",
    "            accuracy_test[sub,fold,:] = model.history[:, 'test_acc']\n",
    "            # result[sub,fold] =model.score(test_set, test_label)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "savemat('accuracy_datasetII_ttsnet_62chn_2e3_500.mat', {'accuracy':accuracy_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sub0:: 100%|██████████| 5/5 [29:04<00:00, 348.98s/it]\n",
      "Sub1:: 100%|██████████| 5/5 [29:01<00:00, 348.34s/it]\n",
      "Sub2:: 100%|██████████| 5/5 [28:35<00:00, 343.09s/it]\n",
      "Sub3:: 100%|██████████| 5/5 [29:02<00:00, 348.52s/it]\n",
      "Sub4:: 100%|██████████| 5/5 [29:04<00:00, 348.87s/it]\n",
      "Sub5:: 100%|██████████| 5/5 [29:08<00:00, 349.62s/it]\n",
      "Sub6:: 100%|██████████| 5/5 [29:00<00:00, 348.20s/it]\n",
      "Sub7:: 100%|██████████| 5/5 [29:09<00:00, 349.83s/it]\n",
      "Sub8:: 100%|██████████| 5/5 [29:10<00:00, 350.16s/it]\n",
      "Sub9:: 100%|██████████| 5/5 [29:00<00:00, 348.11s/it]\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    Fs = 256\n",
    "    num_fold = 5\n",
    "    num_subj = 10\n",
    "    num_epoch= 500\n",
    "    num_class= 5\n",
    "    lr_rate = 0.002\n",
    "    max_freq = 10\n",
    "    step_bank= 2\n",
    "    frange = np.arange(step_bank,max_freq+1,step_bank)\n",
    "    channels = np.arange(62)#(np.int64([1,3,5,15,29,30,31,45,55,57,59])-1).tolist()\n",
    "    num_bank = int(max_freq/step_bank)\n",
    "    t  = [0, 2]\n",
    "    accuracy_test  = np.zeros([num_subj, num_fold, num_epoch])\n",
    "    torch.manual_seed(1169456885)\n",
    "    np.random.seed(1169456885)\n",
    "    for sub in range(num_subj):\n",
    "        dataset=Dataset(num_class=num_class,path='Dataset/OData/Dataset II/', channel=channels)\n",
    "        dataset.load_data(sub, None)\n",
    "        dataset.divide_data(num_fold)\n",
    "        dataset.filter_data(fs=Fs, frange=frange, order=8)\n",
    "        for fold in tqdm(range(num_fold), desc=f'Sub{sub}:'):\n",
    "            train_set, train_label, test_set, test_label = dataset.retreive_data(\n",
    "                    class_index=np.arange(num_class), fold_index=fold, filter_index=np.arange(num_bank)+1, precison='*')\n",
    "            train_set = np.transpose(train_set[:,:,int(t[0]*Fs):int(t[1]*Fs),:], (0,3,1,2))\n",
    "            test_set  = np.transpose(test_set[:,:,int(t[0]*Fs):int(t[1]*Fs), :], (0,3,1,2))\n",
    "            net = TTSNet(num_classes=num_class, num_samples=int(t[1]-t[0])*Fs, num_fbanks=num_bank, num_channels=len(channels))\n",
    "            model = NeuralNetClassifier(\n",
    "                net,\n",
    "                max_epochs=num_epoch,\n",
    "                criterion=TTSLoss(),\n",
    "                lr=lr_rate,\n",
    "                optimizer=torch.optim.Adam,\n",
    "                device='cuda',\n",
    "                callbacks=[EpochScoringonTest(test_set, test_label, scoring='accuracy', name='test_acc',\n",
    "                                              on_train=False)]\n",
    "            )\n",
    "            model.set_params(train_split=None, verbose=0)\n",
    "            model.fit(train_set, train_label)\n",
    "            accuracy_test[sub,fold,:] = model.history[:, 'test_acc']\n",
    "            # result[sub,fold] =model.score(test_set, test_label)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "savemat('accuracy_datasetII_ttsnet_62chn_2e3_500_nz.mat', {'accuracy':accuracy_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20ec9fb4a00>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.mean(accuracy_test, axis=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "data = loadmat('accuracy_datasetII_ttsnet_62chn_2e3_500.mat')\n",
    "accuracy_test11 = data['accuracy']\n",
    "data = loadmat('accuracy_datasetII_ttsnet_62chn_2e3_500_nz.mat')\n",
    "accuracy_test61 = data['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20e92829910>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.mean(accuracy_test11[:15,:,:], axis=(0,1)))\n",
    "plt.plot(np.mean(accuracy_test61[:15,:,:], axis=(0,1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
